{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "#from vit_pytorch.t2t import T2TViT\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_and_loaders():\n",
    "    # Base transform\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Augmentations\n",
    "    augmentations = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2)\n",
    "    ])\n",
    "\n",
    "    # Load the original dataset\n",
    "    original_dataset = torchvision.datasets.ImageFolder(root=r'C:\\Users\\CoolA\\Code\\Medicinal Leaves\\Dataset 1\\train', transform=base_transform)\n",
    "\n",
    "    # Create an augmented dataset\n",
    "    augmented_dataset = torchvision.datasets.ImageFolder(root=r'C:\\Users\\CoolA\\Code\\Medicinal Leaves\\Dataset 1\\train', transform=augmentations)\n",
    "\n",
    "    # Combine the original and augmented datasets\n",
    "    trainset = ConcatDataset([original_dataset, augmented_dataset])\n",
    "\n",
    "    # Create data loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "    valset = torchvision.datasets.ImageFolder(root=r'C:\\Users\\CoolA\\Code\\Medicinal Leaves\\Dataset 1\\val', transform=base_transform)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, valloader, original_dataset.classes\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader, valloader, classes = create_datasets_and_loaders()\n",
    "num_classes = len(classes)\n",
    "results_log = []\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, trainloader, valloader, criterion, optimizer, num_epochs=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # print(f\"Inputs shape: {inputs.shape}\")\n",
    "            # print(f\"Labels shape: {labels.shape}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(f\"Outputs shape: {outputs.shape}\")\n",
    "            \n",
    "            assert outputs.shape == (inputs.shape[0], num_classes), f\"Expected output shape {(inputs.shape[0], num_classes)}, but got {outputs.shape}\"\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if isinstance(outputs, tuple):\n",
    "                _, predicted = outputs[0].max(1)\n",
    "            else:\n",
    "                _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100. * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                if isinstance(outputs, tuple):\n",
    "                    _, predicted = outputs[0].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print()\n",
    "        \n",
    "        results.append([epoch+1, train_loss, train_acc, val_loss, val_acc])\n",
    "        \n",
    "    return [model_name, results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large ViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT parameters: 85821726\n",
      "Epoch 1/20:\n",
      "Train Loss: 3.4262, Train Acc: 8.45%\n",
      "Val Loss: 2.7533, Val Acc: 18.95%\n",
      "\n",
      "Epoch 2/20:\n",
      "Train Loss: 2.7908, Train Acc: 18.59%\n",
      "Val Loss: 2.1373, Val Acc: 30.26%\n",
      "\n",
      "Epoch 3/20:\n",
      "Train Loss: 2.3352, Train Acc: 30.72%\n",
      "Val Loss: 1.8782, Val Acc: 41.84%\n",
      "\n",
      "Epoch 4/20:\n",
      "Train Loss: 2.1502, Train Acc: 35.84%\n",
      "Val Loss: 1.7751, Val Acc: 44.74%\n",
      "\n",
      "Epoch 5/20:\n",
      "Train Loss: 1.9907, Train Acc: 39.04%\n",
      "Val Loss: 1.5256, Val Acc: 52.11%\n",
      "\n",
      "Epoch 6/20:\n",
      "Train Loss: 1.8434, Train Acc: 43.95%\n",
      "Val Loss: 1.4803, Val Acc: 53.16%\n",
      "\n",
      "Epoch 7/20:\n",
      "Train Loss: 1.7927, Train Acc: 46.22%\n",
      "Val Loss: 1.4137, Val Acc: 54.47%\n",
      "\n",
      "Epoch 8/20:\n",
      "Train Loss: 1.6264, Train Acc: 51.13%\n",
      "Val Loss: 1.5081, Val Acc: 53.95%\n",
      "\n",
      "Epoch 9/20:\n",
      "Train Loss: 1.6489, Train Acc: 50.62%\n",
      "Val Loss: 1.2181, Val Acc: 63.42%\n",
      "\n",
      "Epoch 10/20:\n",
      "Train Loss: 1.6048, Train Acc: 52.16%\n",
      "Val Loss: 1.1054, Val Acc: 66.58%\n",
      "\n",
      "Epoch 11/20:\n",
      "Train Loss: 1.6427, Train Acc: 50.58%\n",
      "Val Loss: 1.3287, Val Acc: 56.84%\n",
      "\n",
      "Epoch 12/20:\n",
      "Train Loss: 1.5728, Train Acc: 54.12%\n",
      "Val Loss: 1.3328, Val Acc: 60.79%\n",
      "\n",
      "Epoch 13/20:\n",
      "Train Loss: 1.5352, Train Acc: 55.02%\n",
      "Val Loss: 1.0914, Val Acc: 62.89%\n",
      "\n",
      "Epoch 14/20:\n",
      "Train Loss: 1.4694, Train Acc: 55.57%\n",
      "Val Loss: 1.0791, Val Acc: 65.00%\n",
      "\n",
      "Epoch 15/20:\n",
      "Train Loss: 1.5306, Train Acc: 54.74%\n",
      "Val Loss: 1.1382, Val Acc: 62.37%\n",
      "\n",
      "Epoch 16/20:\n",
      "Train Loss: 1.4769, Train Acc: 55.19%\n",
      "Val Loss: 0.9838, Val Acc: 67.89%\n",
      "\n",
      "Epoch 17/20:\n",
      "Train Loss: 1.3245, Train Acc: 58.93%\n",
      "Val Loss: 0.9524, Val Acc: 69.21%\n",
      "\n",
      "Epoch 18/20:\n",
      "Train Loss: 1.3973, Train Acc: 57.97%\n",
      "Val Loss: 0.9812, Val Acc: 73.68%\n",
      "\n",
      "Epoch 19/20:\n",
      "Train Loss: 1.3056, Train Acc: 60.07%\n",
      "Val Loss: 1.0843, Val Acc: 67.63%\n",
      "\n",
      "Epoch 20/20:\n",
      "Train Loss: 1.5087, Train Acc: 54.91%\n",
      "Val Loss: 1.4298, Val Acc: 58.95%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ViT (Vision Transformer)\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "print(f\"ViT parameters: {count_parameters(model)}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"ViT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeiT (Data-efficient image Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(torch.nn.Module):\n",
    "    def __init__(self, base_criterion: torch.nn.Module, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs, outputs_kd = outputs\n",
    "            loss = self.base_criterion(outputs, labels)\n",
    "            loss_kd = self.base_criterion(outputs_kd, labels)\n",
    "            loss = self.alpha * loss + (1 - self.alpha) * loss_kd\n",
    "        else:\n",
    "            loss = self.base_criterion(outputs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DeiT (Data-efficient image Transformers)\n",
    "model = create_model('deit_base_distilled_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "num_ftrs = model.head_dist.in_features\n",
    "model.head_dist = torch.nn.Linear(num_ftrs, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT parameters: 85846332\n",
      "Epoch 1/20:\n",
      "Train Loss: 2.3864, Train Acc: 29.18%\n",
      "Val Loss: 1.5010, Val Acc: 51.84%\n",
      "\n",
      "Epoch 2/20:\n",
      "Train Loss: 1.4306, Train Acc: 56.98%\n",
      "Val Loss: 0.9763, Val Acc: 68.42%\n",
      "\n",
      "Epoch 3/20:\n",
      "Train Loss: 1.0156, Train Acc: 68.35%\n",
      "Val Loss: 0.6840, Val Acc: 76.84%\n",
      "\n",
      "Epoch 4/20:\n",
      "Train Loss: 0.7993, Train Acc: 75.88%\n",
      "Val Loss: 0.5709, Val Acc: 82.89%\n",
      "\n",
      "Epoch 5/20:\n",
      "Train Loss: 0.6460, Train Acc: 80.38%\n",
      "Val Loss: 0.4616, Val Acc: 86.58%\n",
      "\n",
      "Epoch 6/20:\n",
      "Train Loss: 0.6135, Train Acc: 81.27%\n",
      "Val Loss: 0.4543, Val Acc: 85.53%\n",
      "\n",
      "Epoch 7/20:\n",
      "Train Loss: 0.5646, Train Acc: 82.82%\n",
      "Val Loss: 0.3697, Val Acc: 89.21%\n",
      "\n",
      "Epoch 8/20:\n",
      "Train Loss: 0.4886, Train Acc: 84.67%\n",
      "Val Loss: 0.2753, Val Acc: 91.58%\n",
      "\n",
      "Epoch 9/20:\n",
      "Train Loss: 0.5024, Train Acc: 84.33%\n",
      "Val Loss: 0.3525, Val Acc: 90.26%\n",
      "\n",
      "Epoch 10/20:\n",
      "Train Loss: 0.4741, Train Acc: 84.74%\n",
      "Val Loss: 0.4697, Val Acc: 85.00%\n",
      "\n",
      "Epoch 11/20:\n",
      "Train Loss: 0.4527, Train Acc: 85.60%\n",
      "Val Loss: 0.3365, Val Acc: 90.79%\n",
      "\n",
      "Epoch 12/20:\n",
      "Train Loss: 0.4359, Train Acc: 85.88%\n",
      "Val Loss: 0.2633, Val Acc: 91.32%\n",
      "\n",
      "Epoch 13/20:\n",
      "Train Loss: 0.3863, Train Acc: 88.59%\n",
      "Val Loss: 0.3270, Val Acc: 90.00%\n",
      "\n",
      "Epoch 14/20:\n",
      "Train Loss: 0.3763, Train Acc: 88.42%\n",
      "Val Loss: 0.2564, Val Acc: 92.37%\n",
      "\n",
      "Epoch 15/20:\n",
      "Train Loss: 0.4152, Train Acc: 87.15%\n",
      "Val Loss: 0.2365, Val Acc: 93.68%\n",
      "\n",
      "Epoch 16/20:\n",
      "Train Loss: 0.3327, Train Acc: 89.97%\n",
      "Val Loss: 0.2092, Val Acc: 92.89%\n",
      "\n",
      "Epoch 17/20:\n",
      "Train Loss: 0.3273, Train Acc: 90.38%\n",
      "Val Loss: 0.2539, Val Acc: 91.58%\n",
      "\n",
      "Epoch 18/20:\n",
      "Train Loss: 0.3023, Train Acc: 90.41%\n",
      "Val Loss: 0.1731, Val Acc: 95.53%\n",
      "\n",
      "Epoch 19/20:\n",
      "Train Loss: 0.3024, Train Acc: 90.27%\n",
      "Val Loss: 0.2828, Val Acc: 91.58%\n",
      "\n",
      "Epoch 20/20:\n",
      "Train Loss: 0.3202, Train Acc: 89.73%\n",
      "Val Loss: 0.2449, Val Acc: 91.32%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"DeiT parameters: {count_parameters(model)}\")\n",
    "\n",
    "base_criterion = nn.CrossEntropyLoss()\n",
    "criterion = DistillationLoss(base_criterion, alpha=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"DeiT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1024\n",
      "Swin Transformer parameters: 87798974\n"
     ]
    }
   ],
   "source": [
    "class SwinTransformerForClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(base_model.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.forward_features(x)\n",
    "        #print(f\"After base_model: {x.shape}\")\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=[1, 2])\n",
    "        #print(f\"After global average pooling: {x.shape}\")\n",
    "        \n",
    "        # Final classification layer\n",
    "        x = self.fc(x)\n",
    "        #print(f\"After fc: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the base model\n",
    "base_model = create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "\n",
    "# Print the number of features\n",
    "print(f\"Number of features: {base_model.num_features}\")\n",
    "\n",
    "# Create our custom model\n",
    "model = SwinTransformerForClassification(base_model, num_classes)\n",
    "\n",
    "print(f\"Swin Transformer parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformerForClassification(\n",
       "  (base_model): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.004)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.009)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.013)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.013)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.017)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.022)\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.026)\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.030)\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.035)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.035)\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.039)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.039)\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.043)\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.048)\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.052)\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.057)\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.061)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.061)\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.065)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.065)\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.070)\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.074)\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.078)\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.083)\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.087)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.087)\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.091)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.096)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=1024, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "Train Loss: 3.4307, Train Acc: 5.19%\n",
      "Val Loss: 3.3874, Val Acc: 6.58%\n",
      "\n",
      "Epoch 2/20:\n",
      "Train Loss: 3.2405, Train Acc: 8.01%\n",
      "Val Loss: 3.0361, Val Acc: 12.11%\n",
      "\n",
      "Epoch 3/20:\n",
      "Train Loss: 3.1039, Train Acc: 9.83%\n",
      "Val Loss: 3.0052, Val Acc: 11.32%\n",
      "\n",
      "Epoch 4/20:\n",
      "Train Loss: 3.0788, Train Acc: 11.31%\n",
      "Val Loss: 2.9316, Val Acc: 14.47%\n",
      "\n",
      "Epoch 5/20:\n",
      "Train Loss: 3.2557, Train Acc: 7.56%\n",
      "Val Loss: 3.3973, Val Acc: 3.95%\n",
      "\n",
      "Epoch 6/20:\n",
      "Train Loss: 3.3926, Train Acc: 5.91%\n",
      "Val Loss: 3.3783, Val Acc: 5.26%\n",
      "\n",
      "Epoch 7/20:\n",
      "Train Loss: 3.3905, Train Acc: 5.46%\n",
      "Val Loss: 3.3782, Val Acc: 4.21%\n",
      "\n",
      "Epoch 8/20:\n",
      "Train Loss: 3.3790, Train Acc: 6.25%\n",
      "Val Loss: 3.3768, Val Acc: 6.58%\n",
      "\n",
      "Epoch 9/20:\n",
      "Train Loss: 3.3796, Train Acc: 6.12%\n",
      "Val Loss: 3.3809, Val Acc: 6.58%\n",
      "\n",
      "Epoch 10/20:\n",
      "Train Loss: 3.3834, Train Acc: 6.94%\n",
      "Val Loss: 3.3716, Val Acc: 5.53%\n",
      "\n",
      "Epoch 11/20:\n",
      "Train Loss: 3.3752, Train Acc: 5.74%\n",
      "Val Loss: 3.3693, Val Acc: 6.58%\n",
      "\n",
      "Epoch 12/20:\n",
      "Train Loss: 3.3751, Train Acc: 5.91%\n",
      "Val Loss: 3.3658, Val Acc: 6.58%\n",
      "\n",
      "Epoch 13/20:\n",
      "Train Loss: 3.3731, Train Acc: 6.39%\n",
      "Val Loss: 3.3719, Val Acc: 6.58%\n",
      "\n",
      "Epoch 14/20:\n",
      "Train Loss: 3.3737, Train Acc: 6.60%\n",
      "Val Loss: 3.3671, Val Acc: 5.53%\n",
      "\n",
      "Epoch 15/20:\n",
      "Train Loss: 3.3680, Train Acc: 6.53%\n",
      "Val Loss: 3.3695, Val Acc: 6.58%\n",
      "\n",
      "Epoch 16/20:\n",
      "Train Loss: 3.3709, Train Acc: 6.63%\n",
      "Val Loss: 3.3725, Val Acc: 5.53%\n",
      "\n",
      "Epoch 17/20:\n",
      "Train Loss: 3.3680, Train Acc: 6.32%\n",
      "Val Loss: 3.3646, Val Acc: 6.58%\n",
      "\n",
      "Epoch 18/20:\n",
      "Train Loss: 3.3680, Train Acc: 6.70%\n",
      "Val Loss: 3.3876, Val Acc: 5.26%\n",
      "\n",
      "Epoch 19/20:\n",
      "Train Loss: 3.3709, Train Acc: 6.39%\n",
      "Val Loss: 3.3743, Val Acc: 6.58%\n",
      "\n",
      "Epoch 20/20:\n",
      "Train Loss: 3.3666, Train Acc: 6.19%\n",
      "Val Loss: 3.3814, Val Acc: 6.58%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"SwinT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mobile ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViTForClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.base_model.head = nn.Identity()  # Remove the original classification head\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(base_model.num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.global_pool(x).flatten(1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileViT parameters: 4956862\n",
      "Epoch 1/20:\n",
      "Train Loss: 1.1408, Train Acc: 78.59%\n",
      "Val Loss: 0.2079, Val Acc: 93.68%\n",
      "\n",
      "Epoch 2/20:\n",
      "Train Loss: 0.2824, Train Acc: 92.51%\n",
      "Val Loss: 0.0504, Val Acc: 99.21%\n",
      "\n",
      "Epoch 3/20:\n",
      "Train Loss: 0.1980, Train Acc: 94.36%\n",
      "Val Loss: 0.0230, Val Acc: 99.74%\n",
      "\n",
      "Epoch 4/20:\n",
      "Train Loss: 0.1226, Train Acc: 96.60%\n",
      "Val Loss: 0.0267, Val Acc: 99.21%\n",
      "\n",
      "Epoch 5/20:\n",
      "Train Loss: 0.1249, Train Acc: 96.19%\n",
      "Val Loss: 0.0492, Val Acc: 98.42%\n",
      "\n",
      "Epoch 6/20:\n",
      "Train Loss: 0.1407, Train Acc: 96.19%\n",
      "Val Loss: 0.0106, Val Acc: 100.00%\n",
      "\n",
      "Epoch 7/20:\n",
      "Train Loss: 0.1147, Train Acc: 96.49%\n",
      "Val Loss: 0.2454, Val Acc: 92.63%\n",
      "\n",
      "Epoch 8/20:\n",
      "Train Loss: 0.1029, Train Acc: 97.18%\n",
      "Val Loss: 0.1065, Val Acc: 95.53%\n",
      "\n",
      "Epoch 9/20:\n",
      "Train Loss: 0.0878, Train Acc: 97.42%\n",
      "Val Loss: 0.0054, Val Acc: 100.00%\n",
      "\n",
      "Epoch 10/20:\n",
      "Train Loss: 0.0830, Train Acc: 97.84%\n",
      "Val Loss: 0.0232, Val Acc: 99.74%\n",
      "\n",
      "Epoch 11/20:\n",
      "Train Loss: 0.0706, Train Acc: 98.25%\n",
      "Val Loss: 0.0099, Val Acc: 99.74%\n",
      "\n",
      "Epoch 12/20:\n",
      "Train Loss: 0.0741, Train Acc: 97.66%\n",
      "Val Loss: 0.0343, Val Acc: 99.21%\n",
      "\n",
      "Epoch 13/20:\n",
      "Train Loss: 0.0894, Train Acc: 97.18%\n",
      "Val Loss: 0.0339, Val Acc: 98.95%\n",
      "\n",
      "Epoch 14/20:\n",
      "Train Loss: 0.0797, Train Acc: 97.80%\n",
      "Val Loss: 0.0220, Val Acc: 99.47%\n",
      "\n",
      "Epoch 15/20:\n",
      "Train Loss: 0.0981, Train Acc: 97.11%\n",
      "Val Loss: 0.0340, Val Acc: 98.68%\n",
      "\n",
      "Epoch 16/20:\n",
      "Train Loss: 0.0822, Train Acc: 97.63%\n",
      "Val Loss: 0.0542, Val Acc: 97.89%\n",
      "\n",
      "Epoch 17/20:\n",
      "Train Loss: 0.0736, Train Acc: 98.11%\n",
      "Val Loss: 0.0267, Val Acc: 98.68%\n",
      "\n",
      "Epoch 18/20:\n",
      "Train Loss: 0.0884, Train Acc: 97.56%\n",
      "Val Loss: 0.0569, Val Acc: 98.42%\n",
      "\n",
      "Epoch 19/20:\n",
      "Train Loss: 0.0698, Train Acc: 98.04%\n",
      "Val Loss: 0.0412, Val Acc: 98.95%\n",
      "\n",
      "Epoch 20/20:\n",
      "Train Loss: 0.0648, Train Acc: 97.94%\n",
      "Val Loss: 0.0254, Val Acc: 98.95%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the base model\n",
    "base_model = create_model('mobilevit_s', pretrained=True)\n",
    "# Create the custom model\n",
    "model = MobileViTForClassification(base_model, num_classes)\n",
    "\n",
    "print(f\"MobileViT parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"MobileViT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact Vision Transformers (CVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unknown model (cvt_13_224)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 5. Compact Vision Transformers (CVT)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcvt_13_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39min_features, num_classes)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCVT parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_parameters(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CoolA\\Code\\.venv\\Lib\\site-packages\\timm\\models\\_factory.py:113\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         pretrained_cfg \u001b[38;5;241m=\u001b[39m pretrained_tag\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model(model_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown model (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_name)\n\u001b[0;32m    115\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unknown model (cvt_13_224)"
     ]
    }
   ],
   "source": [
    "# 5. Compact Vision Transformers (CVT)\n",
    "model = create_model('cvt_13_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "print(f\"CVT parameters: {count_parameters(model)}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"CVT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pyramid Vision Transformer (PVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PVT parameters: 24865246\n",
      "Epoch 1/20:\n",
      "Train Loss: 3.4051, Train Acc: 5.43%\n",
      "Val Loss: 3.2279, Val Acc: 6.32%\n",
      "\n",
      "Epoch 2/20:\n",
      "Train Loss: 3.1732, Train Acc: 9.48%\n",
      "Val Loss: 2.8935, Val Acc: 13.68%\n",
      "\n",
      "Epoch 3/20:\n",
      "Train Loss: 3.2431, Train Acc: 9.07%\n",
      "Val Loss: 3.3816, Val Acc: 6.58%\n",
      "\n",
      "Epoch 4/20:\n",
      "Train Loss: 3.3812, Train Acc: 6.19%\n",
      "Val Loss: 3.3782, Val Acc: 6.58%\n",
      "\n",
      "Epoch 5/20:\n",
      "Train Loss: 3.3786, Train Acc: 5.84%\n",
      "Val Loss: 3.3779, Val Acc: 6.58%\n",
      "\n",
      "Epoch 6/20:\n",
      "Train Loss: 3.3803, Train Acc: 5.95%\n",
      "Val Loss: 3.3773, Val Acc: 6.58%\n",
      "\n",
      "Epoch 7/20:\n",
      "Train Loss: 3.3748, Train Acc: 5.91%\n",
      "Val Loss: 3.3708, Val Acc: 6.58%\n",
      "\n",
      "Epoch 8/20:\n",
      "Train Loss: 3.3750, Train Acc: 6.67%\n",
      "Val Loss: 3.3707, Val Acc: 6.58%\n",
      "\n",
      "Epoch 9/20:\n",
      "Train Loss: 3.3725, Train Acc: 6.63%\n",
      "Val Loss: 3.3699, Val Acc: 6.58%\n",
      "\n",
      "Epoch 10/20:\n",
      "Train Loss: 3.3711, Train Acc: 6.43%\n",
      "Val Loss: 3.3668, Val Acc: 5.53%\n",
      "\n",
      "Epoch 11/20:\n",
      "Train Loss: 3.3729, Train Acc: 6.12%\n",
      "Val Loss: 3.3702, Val Acc: 5.53%\n",
      "\n",
      "Epoch 12/20:\n",
      "Train Loss: 3.3677, Train Acc: 6.60%\n",
      "Val Loss: 3.3689, Val Acc: 6.58%\n",
      "\n",
      "Epoch 13/20:\n",
      "Train Loss: 3.3718, Train Acc: 6.53%\n",
      "Val Loss: 3.3652, Val Acc: 6.58%\n",
      "\n",
      "Epoch 14/20:\n",
      "Train Loss: 3.3668, Train Acc: 6.25%\n",
      "Val Loss: 3.3677, Val Acc: 6.58%\n",
      "\n",
      "Epoch 15/20:\n",
      "Train Loss: 3.3693, Train Acc: 6.67%\n",
      "Val Loss: 3.3643, Val Acc: 6.58%\n",
      "\n",
      "Epoch 16/20:\n",
      "Train Loss: 3.3672, Train Acc: 6.67%\n",
      "Val Loss: 3.3627, Val Acc: 6.58%\n",
      "\n",
      "Epoch 17/20:\n",
      "Train Loss: 3.3666, Train Acc: 6.67%\n",
      "Val Loss: 3.3659, Val Acc: 6.58%\n",
      "\n",
      "Epoch 18/20:\n",
      "Train Loss: 3.3656, Train Acc: 6.67%\n",
      "Val Loss: 3.3701, Val Acc: 5.53%\n",
      "\n",
      "Epoch 19/20:\n",
      "Train Loss: 3.3647, Train Acc: 6.77%\n",
      "Val Loss: 3.3704, Val Acc: 6.58%\n",
      "\n",
      "Epoch 20/20:\n",
      "Train Loss: 3.3641, Train Acc: 6.43%\n",
      "Val Loss: 3.3684, Val Acc: 6.58%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. PVT (Pyramid Vision Transformer)\n",
    "model = create_model('pvt_v2_b2', pretrained=True, num_classes=num_classes)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "print(f\"PVT parameters: {count_parameters(model)}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"PVT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Token-to-Token ViT (T2T-ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unknown model (t2t_vit_14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 7. T2T-ViT (Tokens-to-Token ViT)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt2t_vit_14\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT2T-ViT parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_parameters(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CoolA\\Code\\.venv\\Lib\\site-packages\\timm\\models\\_factory.py:113\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         pretrained_cfg \u001b[38;5;241m=\u001b[39m pretrained_tag\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model(model_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown model (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_name)\n\u001b[0;32m    115\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unknown model (t2t_vit_14)"
     ]
    }
   ],
   "source": [
    "# 7. T2T-ViT (Tokens-to-Token ViT)\n",
    "model = create_model('t2t_vit_14', pretrained=False, num_classes=num_classes)\n",
    "print(f\"T2T-ViT parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2T-ViT parameters: 22968376\n",
      "Epoch 1/10:\n",
      "Train Loss: 3.2855, Train Acc: 8.80%\n",
      "Val Loss: 3.4034, Val Acc: 11.32%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 3.2154, Train Acc: 9.83%\n",
      "Val Loss: 2.9995, Val Acc: 8.95%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 3.2525, Train Acc: 8.69%\n",
      "Val Loss: 3.1465, Val Acc: 12.11%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 3.1436, Train Acc: 10.45%\n",
      "Val Loss: 3.4469, Val Acc: 9.21%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 3.1009, Train Acc: 12.78%\n",
      "Val Loss: 2.9321, Val Acc: 18.42%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 3.1960, Train Acc: 11.27%\n",
      "Val Loss: 3.0214, Val Acc: 13.16%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 3.1755, Train Acc: 11.55%\n",
      "Val Loss: 3.0381, Val Acc: 15.53%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 3.1190, Train Acc: 12.44%\n",
      "Val Loss: 2.9607, Val Acc: 15.00%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 3.2908, Train Acc: 8.25%\n",
      "Val Loss: 3.4020, Val Acc: 6.32%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 3.3653, Train Acc: 7.18%\n",
      "Val Loss: 3.3360, Val Acc: 6.84%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results_log.append(train_model(model, \"T2TViT\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MobileViT',\n",
      "  [[1,\n",
      "    1.1322349488407701,\n",
      "    78.48797250859107,\n",
      "    0.38796663253257674,\n",
      "    88.15789473684211],\n",
      "   [2,\n",
      "    0.27744451312573404,\n",
      "    92.92096219931271,\n",
      "    0.3815717758843675,\n",
      "    90.52631578947368],\n",
      "   [3,\n",
      "    0.1903810644379029,\n",
      "    94.81099656357388,\n",
      "    0.06363874473997082,\n",
      "    98.6842105263158],\n",
      "   [4,\n",
      "    0.15827544677265726,\n",
      "    95.63573883161511,\n",
      "    0.1497575838099389,\n",
      "    96.84210526315789],\n",
      "   [5,\n",
      "    0.13721451036386437,\n",
      "    96.49484536082474,\n",
      "    0.11183129770021576,\n",
      "    95.78947368421052],\n",
      "   [6, 0.10646251993315232, 97.18213058419244, 0.006984180969690594, 100.0],\n",
      "   [7,\n",
      "    0.08577700416240228,\n",
      "    97.80068728522336,\n",
      "    0.1039104358642362,\n",
      "    96.57894736842105],\n",
      "   [8,\n",
      "    0.1368462966964836,\n",
      "    96.08247422680412,\n",
      "    0.15292733601139238,\n",
      "    95.26315789473684],\n",
      "   [9,\n",
      "    0.10655207334311453,\n",
      "    96.90721649484536,\n",
      "    0.035926568671129644,\n",
      "    98.6842105263158],\n",
      "   [10,\n",
      "    0.09183610201516486,\n",
      "    97.38831615120274,\n",
      "    0.03306130847583214,\n",
      "    99.21052631578948]]],\n",
      " ['PVT',\n",
      "  [[1,\n",
      "    3.3949635369437083,\n",
      "    5.979381443298969,\n",
      "    3.093595782915751,\n",
      "    9.473684210526315],\n",
      "   [2,\n",
      "    2.9253663466526914,\n",
      "    12.88659793814433,\n",
      "    2.3686427672704062,\n",
      "    24.210526315789473],\n",
      "   [3,\n",
      "    2.54745163498344,\n",
      "    23.436426116838486,\n",
      "    2.1708924770355225,\n",
      "    29.473684210526315],\n",
      "   [4,\n",
      "    2.6156081681723125,\n",
      "    21.89003436426117,\n",
      "    2.007856080929438,\n",
      "    31.57894736842105],\n",
      "   [5, 2.2782697638312537, 30.652920962199314, 1.8275586267312367, 40.0],\n",
      "   [6,\n",
      "    2.2172673639360365,\n",
      "    31.786941580756015,\n",
      "    1.8118227124214172,\n",
      "    40.526315789473685],\n",
      "   [7,\n",
      "    2.0926686302646176,\n",
      "    35.9106529209622,\n",
      "    1.5988459289073944,\n",
      "    48.68421052631579],\n",
      "   [8,\n",
      "    1.9903441929555201,\n",
      "    38.83161512027491,\n",
      "    1.5668796400229137,\n",
      "    48.94736842105263],\n",
      "   [9,\n",
      "    1.9827535165535224,\n",
      "    39.65635738831615,\n",
      "    1.3956893384456635,\n",
      "    57.36842105263158],\n",
      "   [10,\n",
      "    1.7851243019104004,\n",
      "    45.292096219931274,\n",
      "    1.2598540931940079,\n",
      "    61.31578947368421]]],\n",
      " ['DeiT-Tiny',\n",
      "  [[1,\n",
      "    2.829417118659386,\n",
      "    19.587628865979383,\n",
      "    2.1529143253962197,\n",
      "    34.473684210526315],\n",
      "   [2,\n",
      "    2.0002640708462223,\n",
      "    40.72164948453608,\n",
      "    1.3115047166744869,\n",
      "    57.63157894736842],\n",
      "   [3,\n",
      "    1.6246331484763177,\n",
      "    49.62199312714777,\n",
      "    1.0608934462070465,\n",
      "    70.78947368421052],\n",
      "   [4,\n",
      "    1.1917339327571157,\n",
      "    64.02061855670104,\n",
      "    0.6872871927917004,\n",
      "    75.26315789473684],\n",
      "   [5,\n",
      "    1.0654135130263946,\n",
      "    67.52577319587628,\n",
      "    0.6048671628038088,\n",
      "    80.52631578947368],\n",
      "   [6,\n",
      "    0.8593480085933601,\n",
      "    73.53951890034364,\n",
      "    0.6116355701039234,\n",
      "    81.57894736842105],\n",
      "   [7,\n",
      "    0.8160909096618275,\n",
      "    74.74226804123711,\n",
      "    0.5900363276402155,\n",
      "    82.10526315789474],\n",
      "   [8,\n",
      "    0.7108290722736945,\n",
      "    78.96907216494846,\n",
      "    0.49629574672629434,\n",
      "    83.6842105263158],\n",
      "   [9,\n",
      "    0.6521954374326454,\n",
      "    79.45017182130584,\n",
      "    0.4102009106427431,\n",
      "    87.63157894736842],\n",
      "   [10,\n",
      "    0.5832951894173255,\n",
      "    82.43986254295532,\n",
      "    0.500570194174846,\n",
      "    83.42105263157895]]]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pp(results_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeiT Tiny (5M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(torch.nn.Module):\n",
    "    def __init__(self, base_criterion: torch.nn.Module, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs, outputs_kd = outputs\n",
    "            loss = self.base_criterion(outputs, labels)\n",
    "            loss_kd = self.base_criterion(outputs_kd, labels)\n",
    "            loss = self.alpha * loss + (1 - self.alpha) * loss_kd\n",
    "        else:\n",
    "            loss = self.base_criterion(outputs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96298e59b77446d99e587addc5b0e04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/23.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiT-Tiny parameters: 5536380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformerDistilled(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=192, out_features=30, bias=True)\n",
       "  (head_dist): Linear(in_features=192, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model('deit_tiny_distilled_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.head_dist = nn.Linear(model.head_dist.in_features, num_classes)\n",
    "\n",
    "print(f\"DeiT-Tiny parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 2.8294, Train Acc: 19.59%\n",
      "Val Loss: 2.1529, Val Acc: 34.47%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 2.0003, Train Acc: 40.72%\n",
      "Val Loss: 1.3115, Val Acc: 57.63%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 1.6246, Train Acc: 49.62%\n",
      "Val Loss: 1.0609, Val Acc: 70.79%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 1.1917, Train Acc: 64.02%\n",
      "Val Loss: 0.6873, Val Acc: 75.26%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 1.0654, Train Acc: 67.53%\n",
      "Val Loss: 0.6049, Val Acc: 80.53%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.8593, Train Acc: 73.54%\n",
      "Val Loss: 0.6116, Val Acc: 81.58%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.8161, Train Acc: 74.74%\n",
      "Val Loss: 0.5900, Val Acc: 82.11%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.7108, Train Acc: 78.97%\n",
      "Val Loss: 0.4963, Val Acc: 83.68%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.6522, Train Acc: 79.45%\n",
      "Val Loss: 0.4102, Val Acc: 87.63%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.5833, Train Acc: 82.44%\n",
      "Val Loss: 0.5006, Val Acc: 83.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "base_criterion = nn.CrossEntropyLoss()\n",
    "criterion = DistillationLoss(base_criterion, alpha=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"DeiT-Tiny\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileVIT XXS (0.96M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileViT-XXS parameters: 960654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ByobNet(\n",
       "  (stem): ConvNormAct(\n",
       "    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (shortcut): Identity()\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (shortcut): Identity()\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (shortcut): Identity()\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): MobileVitBlock(\n",
       "        (conv_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_1x1): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (transformer): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_proj): ConvNormAct(\n",
       "          (conv): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_fusion): ConvNormAct(\n",
       "          (conv): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): MobileVitBlock(\n",
       "        (conv_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_1x1): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (transformer): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_proj): ConvNormAct(\n",
       "          (conv): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_fusion): ConvNormAct(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): MobileVitBlock(\n",
       "        (conv_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_1x1): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (transformer): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_proj): ConvNormAct(\n",
       "          (conv): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_fusion): ConvNormAct(\n",
       "          (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): ConvNormAct(\n",
       "    (conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=320, out_features=30, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model('mobilevit_xxs', pretrained=True)\n",
    "in_features = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(in_features, num_classes)\n",
    "#model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "\n",
    "print(f\"MobileViT-XXS parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 1.6459, Train Acc: 65.26%\n",
      "Val Loss: 0.9001, Val Acc: 73.16%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.4847, Train Acc: 88.56%\n",
      "Val Loss: 0.6422, Val Acc: 80.00%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.4080, Train Acc: 88.73%\n",
      "Val Loss: 0.4413, Val Acc: 85.26%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.2647, Train Acc: 93.16%\n",
      "Val Loss: 0.6951, Val Acc: 79.74%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.2469, Train Acc: 93.09%\n",
      "Val Loss: 0.2771, Val Acc: 91.32%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.2028, Train Acc: 94.05%\n",
      "Val Loss: 0.1000, Val Acc: 97.11%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.1921, Train Acc: 94.60%\n",
      "Val Loss: 0.1740, Val Acc: 93.68%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.1621, Train Acc: 95.36%\n",
      "Val Loss: 0.2821, Val Acc: 90.26%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.1432, Train Acc: 95.91%\n",
      "Val Loss: 0.5019, Val Acc: 87.37%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.1244, Train Acc: 96.43%\n",
      "Val Loss: 0.1181, Val Acc: 97.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"MobileViT-XXS\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientFormer-L1 (12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientFormer-L1 parameters: 11418868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientFormer(\n",
       "  (stem): Stem4(\n",
       "    (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (norm1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): EfficientFormerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): EfficientFormerStage(\n",
       "      (downsample): Downsample(\n",
       "        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): EfficientFormerStage(\n",
       "      (downsample): Downsample(\n",
       "        (conv): Conv2d(96, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): EfficientFormerStage(\n",
       "      (downsample): Downsample(\n",
       "        (conv): Conv2d(224, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(1792, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(1792, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): MetaBlock2d(\n",
       "          (token_mixer): Pooling(\n",
       "            (pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "          )\n",
       "          (ls1): LayerScale2d()\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): ConvMlpWithNorm(\n",
       "            (fc1): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(1792, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (norm2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale2d()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Flat()\n",
       "        (4): MetaBlock1d(\n",
       "          (norm1): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "          (token_mixer): Attention(\n",
       "            (qkv): Linear(in_features=448, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=448, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=448, out_features=1792, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (ls1): LayerScale()\n",
       "          (ls2): LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=448, out_features=30, bias=True)\n",
       "  (head_dist): Linear(in_features=448, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model('efficientformer_l1', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.head_dist = nn.Linear(model.head_dist.in_features, num_classes)\n",
    "\n",
    "print(f\"EfficientFormer-L1 parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.7096, Train Acc: 80.34%\n",
      "Val Loss: 0.3074, Val Acc: 91.32%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.2955, Train Acc: 92.37%\n",
      "Val Loss: 1.4206, Val Acc: 60.79%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.3120, Train Acc: 90.96%\n",
      "Val Loss: 0.2710, Val Acc: 93.42%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.1717, Train Acc: 95.26%\n",
      "Val Loss: 0.1001, Val Acc: 97.63%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.1894, Train Acc: 94.54%\n",
      "Val Loss: 0.1351, Val Acc: 94.74%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.1503, Train Acc: 95.81%\n",
      "Val Loss: 0.0277, Val Acc: 98.42%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.1474, Train Acc: 96.05%\n",
      "Val Loss: 0.0231, Val Acc: 99.21%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.1321, Train Acc: 96.32%\n",
      "Val Loss: 0.0928, Val Acc: 97.11%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.1389, Train Acc: 96.25%\n",
      "Val Loss: 0.0834, Val Acc: 97.11%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.1320, Train Acc: 96.36%\n",
      "Val Loss: 0.0304, Val Acc: 98.68%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"EfficientFormer-L1\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNeXt-Atto (3.38M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30e8d6e67ae4abfab5262dbd3b968aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt-Atto parameters: 3384150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 40, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((40,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(40, 40, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=40)\n",
       "          (norm): LayerNorm2d((40,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(40, 40, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=40)\n",
       "          (norm): LayerNorm2d((40,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((40,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(40, 80, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(80, 80, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=80)\n",
       "          (norm): LayerNorm2d((80,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(80, 80, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=80)\n",
       "          (norm): LayerNorm2d((80,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((80,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(80, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
       "          (norm): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
       "          (norm): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
       "          (norm): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
       "          (norm): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
       "          (norm): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160)\n",
       "          (norm): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((160,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(160, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=320)\n",
       "          (norm): LayerNorm2d((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=320)\n",
       "          (norm): LayerNorm2d((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((320,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=320, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model('convnext_atto', pretrained=True)\n",
    "in_features = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "print(f\"ConvNeXt-Atto parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 3.0961, Train Acc: 11.37%\n",
      "Val Loss: 2.4374, Val Acc: 26.32%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 2.3744, Train Acc: 27.08%\n",
      "Val Loss: 1.7738, Val Acc: 44.47%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 1.9699, Train Acc: 40.17%\n",
      "Val Loss: 1.2855, Val Acc: 60.53%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 1.6648, Train Acc: 50.07%\n",
      "Val Loss: 0.9701, Val Acc: 70.79%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 1.4105, Train Acc: 58.45%\n",
      "Val Loss: 0.8148, Val Acc: 76.32%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 1.1872, Train Acc: 63.71%\n",
      "Val Loss: 0.8988, Val Acc: 76.05%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 1.0575, Train Acc: 67.73%\n",
      "Val Loss: 0.7074, Val Acc: 75.53%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.9705, Train Acc: 69.69%\n",
      "Val Loss: 0.5877, Val Acc: 79.74%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.8148, Train Acc: 76.01%\n",
      "Val Loss: 0.4161, Val Acc: 87.37%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.7188, Train Acc: 78.01%\n",
      "Val Loss: 0.3541, Val Acc: 88.95%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"ConvNeXt-Atto\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PVT-Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997e754f12bb4d6194776a68bcea89ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/14.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PVT-Tiny-B0 parameters: 3417470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyramidVisionTransformerV2(\n",
       "  (patch_embed): OverlapPatchEmbed(\n",
       "    (proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): PyramidVisionTransformerStage(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=32, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): PyramidVisionTransformerStage(\n",
       "      (downsample): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (2): PyramidVisionTransformerStage(\n",
       "      (downsample): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (kv): Linear(in_features=160, out_features=320, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (3): PyramidVisionTransformerStage(\n",
       "      (downsample): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x Block(\n",
       "          (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MlpWithDepthwiseConv(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (relu): Identity()\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=256, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model(\"pvt_v2_b0\", pretrained=True, num_classes=30)\n",
    "print(f\"PVT-Tiny-B0 parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 2.4762, Train Acc: 25.81%\n",
      "Val Loss: 1.1276, Val Acc: 62.11%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 1.2685, Train Acc: 59.04%\n",
      "Val Loss: 0.6382, Val Acc: 79.21%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.9597, Train Acc: 68.35%\n",
      "Val Loss: 0.5809, Val Acc: 81.32%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.7084, Train Acc: 77.32%\n",
      "Val Loss: 0.3626, Val Acc: 90.26%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.5436, Train Acc: 82.03%\n",
      "Val Loss: 0.4340, Val Acc: 86.05%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.4251, Train Acc: 86.80%\n",
      "Val Loss: 0.4169, Val Acc: 85.53%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.4276, Train Acc: 86.25%\n",
      "Val Loss: 0.1834, Val Acc: 94.74%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.3250, Train Acc: 89.76%\n",
      "Val Loss: 0.4282, Val Acc: 85.53%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.3990, Train Acc: 87.29%\n",
      "Val Loss: 0.2019, Val Acc: 92.89%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.3055, Train Acc: 90.27%\n",
      "Val Loss: 0.1254, Val Acc: 94.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"pvt_tiny_b0\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny VIT 11M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bat_resnext26ts',\n",
       " 'beit_base_patch16_224',\n",
       " 'beit_base_patch16_384',\n",
       " 'beit_large_patch16_224',\n",
       " 'beit_large_patch16_384',\n",
       " 'beit_large_patch16_512',\n",
       " 'beitv2_base_patch16_224',\n",
       " 'beitv2_large_patch16_224',\n",
       " 'botnet26t_256',\n",
       " 'botnet50ts_256',\n",
       " 'caformer_b36',\n",
       " 'caformer_m36',\n",
       " 'caformer_s18',\n",
       " 'caformer_s36',\n",
       " 'cait_m36_384',\n",
       " 'cait_m48_448',\n",
       " 'cait_s24_224',\n",
       " 'cait_s24_384',\n",
       " 'cait_s36_384',\n",
       " 'cait_xs24_384',\n",
       " 'cait_xxs24_224',\n",
       " 'cait_xxs24_384',\n",
       " 'cait_xxs36_224',\n",
       " 'cait_xxs36_384',\n",
       " 'coat_lite_medium',\n",
       " 'coat_lite_medium_384',\n",
       " 'coat_lite_mini',\n",
       " 'coat_lite_small',\n",
       " 'coat_lite_tiny',\n",
       " 'coat_mini',\n",
       " 'coat_small',\n",
       " 'coat_tiny',\n",
       " 'coatnet_0_224',\n",
       " 'coatnet_0_rw_224',\n",
       " 'coatnet_1_224',\n",
       " 'coatnet_1_rw_224',\n",
       " 'coatnet_2_224',\n",
       " 'coatnet_2_rw_224',\n",
       " 'coatnet_3_224',\n",
       " 'coatnet_3_rw_224',\n",
       " 'coatnet_4_224',\n",
       " 'coatnet_5_224',\n",
       " 'coatnet_bn_0_rw_224',\n",
       " 'coatnet_nano_cc_224',\n",
       " 'coatnet_nano_rw_224',\n",
       " 'coatnet_pico_rw_224',\n",
       " 'coatnet_rmlp_0_rw_224',\n",
       " 'coatnet_rmlp_1_rw2_224',\n",
       " 'coatnet_rmlp_1_rw_224',\n",
       " 'coatnet_rmlp_2_rw_224',\n",
       " 'coatnet_rmlp_2_rw_384',\n",
       " 'coatnet_rmlp_3_rw_224',\n",
       " 'coatnet_rmlp_nano_rw_224',\n",
       " 'coatnext_nano_rw_224',\n",
       " 'convformer_b36',\n",
       " 'convformer_m36',\n",
       " 'convformer_s18',\n",
       " 'convformer_s36',\n",
       " 'convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'convmixer_768_32',\n",
       " 'convmixer_1024_20_ks9_p14',\n",
       " 'convmixer_1536_20',\n",
       " 'convnext_atto',\n",
       " 'convnext_atto_ols',\n",
       " 'convnext_base',\n",
       " 'convnext_femto',\n",
       " 'convnext_femto_ols',\n",
       " 'convnext_large',\n",
       " 'convnext_large_mlp',\n",
       " 'convnext_nano',\n",
       " 'convnext_nano_ols',\n",
       " 'convnext_pico',\n",
       " 'convnext_pico_ols',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'convnext_tiny_hnf',\n",
       " 'convnext_xlarge',\n",
       " 'convnext_xxlarge',\n",
       " 'convnextv2_atto',\n",
       " 'convnextv2_base',\n",
       " 'convnextv2_femto',\n",
       " 'convnextv2_huge',\n",
       " 'convnextv2_large',\n",
       " 'convnextv2_nano',\n",
       " 'convnextv2_pico',\n",
       " 'convnextv2_small',\n",
       " 'convnextv2_tiny',\n",
       " 'crossvit_9_240',\n",
       " 'crossvit_9_dagger_240',\n",
       " 'crossvit_15_240',\n",
       " 'crossvit_15_dagger_240',\n",
       " 'crossvit_15_dagger_408',\n",
       " 'crossvit_18_240',\n",
       " 'crossvit_18_dagger_240',\n",
       " 'crossvit_18_dagger_408',\n",
       " 'crossvit_base_240',\n",
       " 'crossvit_small_240',\n",
       " 'crossvit_tiny_240',\n",
       " 'cs3darknet_focus_l',\n",
       " 'cs3darknet_focus_m',\n",
       " 'cs3darknet_focus_s',\n",
       " 'cs3darknet_focus_x',\n",
       " 'cs3darknet_l',\n",
       " 'cs3darknet_m',\n",
       " 'cs3darknet_s',\n",
       " 'cs3darknet_x',\n",
       " 'cs3edgenet_x',\n",
       " 'cs3se_edgenet_x',\n",
       " 'cs3sedarknet_l',\n",
       " 'cs3sedarknet_x',\n",
       " 'cs3sedarknet_xdw',\n",
       " 'cspdarknet53',\n",
       " 'cspresnet50',\n",
       " 'cspresnet50d',\n",
       " 'cspresnet50w',\n",
       " 'cspresnext50',\n",
       " 'darknet17',\n",
       " 'darknet21',\n",
       " 'darknet53',\n",
       " 'darknetaa53',\n",
       " 'davit_base',\n",
       " 'davit_base_fl',\n",
       " 'davit_giant',\n",
       " 'davit_huge',\n",
       " 'davit_huge_fl',\n",
       " 'davit_large',\n",
       " 'davit_small',\n",
       " 'davit_tiny',\n",
       " 'deit3_base_patch16_224',\n",
       " 'deit3_base_patch16_384',\n",
       " 'deit3_huge_patch14_224',\n",
       " 'deit3_large_patch16_224',\n",
       " 'deit3_large_patch16_384',\n",
       " 'deit3_medium_patch16_224',\n",
       " 'deit3_small_patch16_224',\n",
       " 'deit3_small_patch16_384',\n",
       " 'deit_base_distilled_patch16_224',\n",
       " 'deit_base_distilled_patch16_384',\n",
       " 'deit_base_patch16_224',\n",
       " 'deit_base_patch16_384',\n",
       " 'deit_small_distilled_patch16_224',\n",
       " 'deit_small_patch16_224',\n",
       " 'deit_tiny_distilled_patch16_224',\n",
       " 'deit_tiny_patch16_224',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'densenet264d',\n",
       " 'densenetblur121d',\n",
       " 'dla34',\n",
       " 'dla46_c',\n",
       " 'dla46x_c',\n",
       " 'dla60',\n",
       " 'dla60_res2net',\n",
       " 'dla60_res2next',\n",
       " 'dla60x',\n",
       " 'dla60x_c',\n",
       " 'dla102',\n",
       " 'dla102x',\n",
       " 'dla102x2',\n",
       " 'dla169',\n",
       " 'dm_nfnet_f0',\n",
       " 'dm_nfnet_f1',\n",
       " 'dm_nfnet_f2',\n",
       " 'dm_nfnet_f3',\n",
       " 'dm_nfnet_f4',\n",
       " 'dm_nfnet_f5',\n",
       " 'dm_nfnet_f6',\n",
       " 'dpn48b',\n",
       " 'dpn68',\n",
       " 'dpn68b',\n",
       " 'dpn92',\n",
       " 'dpn98',\n",
       " 'dpn107',\n",
       " 'dpn131',\n",
       " 'eca_botnext26ts_256',\n",
       " 'eca_halonext26ts',\n",
       " 'eca_nfnet_l0',\n",
       " 'eca_nfnet_l1',\n",
       " 'eca_nfnet_l2',\n",
       " 'eca_nfnet_l3',\n",
       " 'eca_resnet33ts',\n",
       " 'eca_resnext26ts',\n",
       " 'eca_vovnet39b',\n",
       " 'ecaresnet26t',\n",
       " 'ecaresnet50d',\n",
       " 'ecaresnet50d_pruned',\n",
       " 'ecaresnet50t',\n",
       " 'ecaresnet101d',\n",
       " 'ecaresnet101d_pruned',\n",
       " 'ecaresnet200d',\n",
       " 'ecaresnet269d',\n",
       " 'ecaresnetlight',\n",
       " 'ecaresnext26t_32x4d',\n",
       " 'ecaresnext50t_32x4d',\n",
       " 'edgenext_base',\n",
       " 'edgenext_small',\n",
       " 'edgenext_small_rw',\n",
       " 'edgenext_x_small',\n",
       " 'edgenext_xx_small',\n",
       " 'efficientformer_l1',\n",
       " 'efficientformer_l3',\n",
       " 'efficientformer_l7',\n",
       " 'efficientformerv2_l',\n",
       " 'efficientformerv2_s0',\n",
       " 'efficientformerv2_s1',\n",
       " 'efficientformerv2_s2',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b0_g8_gn',\n",
       " 'efficientnet_b0_g16_evos',\n",
       " 'efficientnet_b0_gn',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_g8_gn',\n",
       " 'efficientnet_b3_gn',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_b8',\n",
       " 'efficientnet_blur_b0',\n",
       " 'efficientnet_cc_b0_4e',\n",
       " 'efficientnet_cc_b0_8e',\n",
       " 'efficientnet_cc_b1_8e',\n",
       " 'efficientnet_el',\n",
       " 'efficientnet_el_pruned',\n",
       " 'efficientnet_em',\n",
       " 'efficientnet_es',\n",
       " 'efficientnet_es_pruned',\n",
       " 'efficientnet_h_b5',\n",
       " 'efficientnet_l2',\n",
       " 'efficientnet_lite0',\n",
       " 'efficientnet_lite1',\n",
       " 'efficientnet_lite2',\n",
       " 'efficientnet_lite3',\n",
       " 'efficientnet_lite4',\n",
       " 'efficientnet_x_b3',\n",
       " 'efficientnet_x_b5',\n",
       " 'efficientnetv2_l',\n",
       " 'efficientnetv2_m',\n",
       " 'efficientnetv2_rw_m',\n",
       " 'efficientnetv2_rw_s',\n",
       " 'efficientnetv2_rw_t',\n",
       " 'efficientnetv2_s',\n",
       " 'efficientnetv2_xl',\n",
       " 'efficientvit_b0',\n",
       " 'efficientvit_b1',\n",
       " 'efficientvit_b2',\n",
       " 'efficientvit_b3',\n",
       " 'efficientvit_l1',\n",
       " 'efficientvit_l2',\n",
       " 'efficientvit_l3',\n",
       " 'efficientvit_m0',\n",
       " 'efficientvit_m1',\n",
       " 'efficientvit_m2',\n",
       " 'efficientvit_m3',\n",
       " 'efficientvit_m4',\n",
       " 'efficientvit_m5',\n",
       " 'ese_vovnet19b_dw',\n",
       " 'ese_vovnet19b_slim',\n",
       " 'ese_vovnet19b_slim_dw',\n",
       " 'ese_vovnet39b',\n",
       " 'ese_vovnet39b_evos',\n",
       " 'ese_vovnet57b',\n",
       " 'ese_vovnet99b',\n",
       " 'eva02_base_patch14_224',\n",
       " 'eva02_base_patch14_448',\n",
       " 'eva02_base_patch16_clip_224',\n",
       " 'eva02_enormous_patch14_clip_224',\n",
       " 'eva02_large_patch14_224',\n",
       " 'eva02_large_patch14_448',\n",
       " 'eva02_large_patch14_clip_224',\n",
       " 'eva02_large_patch14_clip_336',\n",
       " 'eva02_small_patch14_224',\n",
       " 'eva02_small_patch14_336',\n",
       " 'eva02_tiny_patch14_224',\n",
       " 'eva02_tiny_patch14_336',\n",
       " 'eva_giant_patch14_224',\n",
       " 'eva_giant_patch14_336',\n",
       " 'eva_giant_patch14_560',\n",
       " 'eva_giant_patch14_clip_224',\n",
       " 'eva_large_patch14_196',\n",
       " 'eva_large_patch14_336',\n",
       " 'fastvit_ma36',\n",
       " 'fastvit_mci0',\n",
       " 'fastvit_mci1',\n",
       " 'fastvit_mci2',\n",
       " 'fastvit_s12',\n",
       " 'fastvit_sa12',\n",
       " 'fastvit_sa24',\n",
       " 'fastvit_sa36',\n",
       " 'fastvit_t8',\n",
       " 'fastvit_t12',\n",
       " 'fbnetc_100',\n",
       " 'fbnetv3_b',\n",
       " 'fbnetv3_d',\n",
       " 'fbnetv3_g',\n",
       " 'flexivit_base',\n",
       " 'flexivit_large',\n",
       " 'flexivit_small',\n",
       " 'focalnet_base_lrf',\n",
       " 'focalnet_base_srf',\n",
       " 'focalnet_huge_fl3',\n",
       " 'focalnet_huge_fl4',\n",
       " 'focalnet_large_fl3',\n",
       " 'focalnet_large_fl4',\n",
       " 'focalnet_small_lrf',\n",
       " 'focalnet_small_srf',\n",
       " 'focalnet_tiny_lrf',\n",
       " 'focalnet_tiny_srf',\n",
       " 'focalnet_xlarge_fl3',\n",
       " 'focalnet_xlarge_fl4',\n",
       " 'gc_efficientnetv2_rw_t',\n",
       " 'gcresnet33ts',\n",
       " 'gcresnet50t',\n",
       " 'gcresnext26ts',\n",
       " 'gcresnext50ts',\n",
       " 'gcvit_base',\n",
       " 'gcvit_small',\n",
       " 'gcvit_tiny',\n",
       " 'gcvit_xtiny',\n",
       " 'gcvit_xxtiny',\n",
       " 'gernet_l',\n",
       " 'gernet_m',\n",
       " 'gernet_s',\n",
       " 'ghostnet_050',\n",
       " 'ghostnet_100',\n",
       " 'ghostnet_130',\n",
       " 'ghostnetv2_100',\n",
       " 'ghostnetv2_130',\n",
       " 'ghostnetv2_160',\n",
       " 'gmixer_12_224',\n",
       " 'gmixer_24_224',\n",
       " 'gmlp_b16_224',\n",
       " 'gmlp_s16_224',\n",
       " 'gmlp_ti16_224',\n",
       " 'halo2botnet50ts_256',\n",
       " 'halonet26t',\n",
       " 'halonet50ts',\n",
       " 'halonet_h1',\n",
       " 'haloregnetz_b',\n",
       " 'hardcorenas_a',\n",
       " 'hardcorenas_b',\n",
       " 'hardcorenas_c',\n",
       " 'hardcorenas_d',\n",
       " 'hardcorenas_e',\n",
       " 'hardcorenas_f',\n",
       " 'hgnet_base',\n",
       " 'hgnet_small',\n",
       " 'hgnet_tiny',\n",
       " 'hgnetv2_b0',\n",
       " 'hgnetv2_b1',\n",
       " 'hgnetv2_b2',\n",
       " 'hgnetv2_b3',\n",
       " 'hgnetv2_b4',\n",
       " 'hgnetv2_b5',\n",
       " 'hgnetv2_b6',\n",
       " 'hiera_base_224',\n",
       " 'hiera_base_abswin_256',\n",
       " 'hiera_base_plus_224',\n",
       " 'hiera_huge_224',\n",
       " 'hiera_large_224',\n",
       " 'hiera_small_224',\n",
       " 'hiera_small_abswin_256',\n",
       " 'hiera_tiny_224',\n",
       " 'hieradet_small',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w18_small',\n",
       " 'hrnet_w18_small_v2',\n",
       " 'hrnet_w18_ssld',\n",
       " 'hrnet_w30',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w40',\n",
       " 'hrnet_w44',\n",
       " 'hrnet_w48',\n",
       " 'hrnet_w48_ssld',\n",
       " 'hrnet_w64',\n",
       " 'inception_next_base',\n",
       " 'inception_next_small',\n",
       " 'inception_next_tiny',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'inception_v4',\n",
       " 'lambda_resnet26rpt_256',\n",
       " 'lambda_resnet26t',\n",
       " 'lambda_resnet50ts',\n",
       " 'lamhalobotnet50ts_256',\n",
       " 'lcnet_035',\n",
       " 'lcnet_050',\n",
       " 'lcnet_075',\n",
       " 'lcnet_100',\n",
       " 'lcnet_150',\n",
       " 'legacy_senet154',\n",
       " 'legacy_seresnet18',\n",
       " 'legacy_seresnet34',\n",
       " 'legacy_seresnet50',\n",
       " 'legacy_seresnet101',\n",
       " 'legacy_seresnet152',\n",
       " 'legacy_seresnext26_32x4d',\n",
       " 'legacy_seresnext50_32x4d',\n",
       " 'legacy_seresnext101_32x4d',\n",
       " 'legacy_xception',\n",
       " 'levit_128',\n",
       " 'levit_128s',\n",
       " 'levit_192',\n",
       " 'levit_256',\n",
       " 'levit_256d',\n",
       " 'levit_384',\n",
       " 'levit_384_s8',\n",
       " 'levit_512',\n",
       " 'levit_512_s8',\n",
       " 'levit_512d',\n",
       " 'levit_conv_128',\n",
       " 'levit_conv_128s',\n",
       " 'levit_conv_192',\n",
       " 'levit_conv_256',\n",
       " 'levit_conv_256d',\n",
       " 'levit_conv_384',\n",
       " 'levit_conv_384_s8',\n",
       " 'levit_conv_512',\n",
       " 'levit_conv_512_s8',\n",
       " 'levit_conv_512d',\n",
       " 'maxvit_base_tf_224',\n",
       " 'maxvit_base_tf_384',\n",
       " 'maxvit_base_tf_512',\n",
       " 'maxvit_large_tf_224',\n",
       " 'maxvit_large_tf_384',\n",
       " 'maxvit_large_tf_512',\n",
       " 'maxvit_nano_rw_256',\n",
       " 'maxvit_pico_rw_256',\n",
       " 'maxvit_rmlp_base_rw_224',\n",
       " 'maxvit_rmlp_base_rw_384',\n",
       " 'maxvit_rmlp_nano_rw_256',\n",
       " 'maxvit_rmlp_pico_rw_256',\n",
       " 'maxvit_rmlp_small_rw_224',\n",
       " 'maxvit_rmlp_small_rw_256',\n",
       " 'maxvit_rmlp_tiny_rw_256',\n",
       " 'maxvit_small_tf_224',\n",
       " 'maxvit_small_tf_384',\n",
       " 'maxvit_small_tf_512',\n",
       " 'maxvit_tiny_pm_256',\n",
       " 'maxvit_tiny_rw_224',\n",
       " 'maxvit_tiny_rw_256',\n",
       " 'maxvit_tiny_tf_224',\n",
       " 'maxvit_tiny_tf_384',\n",
       " 'maxvit_tiny_tf_512',\n",
       " 'maxvit_xlarge_tf_224',\n",
       " 'maxvit_xlarge_tf_384',\n",
       " 'maxvit_xlarge_tf_512',\n",
       " 'maxxvit_rmlp_nano_rw_256',\n",
       " 'maxxvit_rmlp_small_rw_256',\n",
       " 'maxxvit_rmlp_tiny_rw_256',\n",
       " 'maxxvitv2_nano_rw_256',\n",
       " 'maxxvitv2_rmlp_base_rw_224',\n",
       " 'maxxvitv2_rmlp_base_rw_384',\n",
       " 'maxxvitv2_rmlp_large_rw_224',\n",
       " 'mixer_b16_224',\n",
       " 'mixer_b32_224',\n",
       " 'mixer_l16_224',\n",
       " 'mixer_l32_224',\n",
       " 'mixer_s16_224',\n",
       " 'mixer_s32_224',\n",
       " 'mixnet_l',\n",
       " 'mixnet_m',\n",
       " 'mixnet_s',\n",
       " 'mixnet_xl',\n",
       " 'mixnet_xxl',\n",
       " 'mnasnet_050',\n",
       " 'mnasnet_075',\n",
       " 'mnasnet_100',\n",
       " 'mnasnet_140',\n",
       " 'mnasnet_small',\n",
       " 'mobilenet_edgetpu_100',\n",
       " 'mobilenet_edgetpu_v2_l',\n",
       " 'mobilenet_edgetpu_v2_m',\n",
       " 'mobilenet_edgetpu_v2_s',\n",
       " 'mobilenet_edgetpu_v2_xs',\n",
       " 'mobilenetv1_100',\n",
       " 'mobilenetv1_100h',\n",
       " 'mobilenetv1_125',\n",
       " 'mobilenetv2_035',\n",
       " 'mobilenetv2_050',\n",
       " 'mobilenetv2_075',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_075',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_large_150d',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_050',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'mobilenetv4_conv_aa_large',\n",
       " 'mobilenetv4_conv_aa_medium',\n",
       " 'mobilenetv4_conv_blur_medium',\n",
       " 'mobilenetv4_conv_large',\n",
       " 'mobilenetv4_conv_medium',\n",
       " 'mobilenetv4_conv_small',\n",
       " 'mobilenetv4_hybrid_large',\n",
       " 'mobilenetv4_hybrid_large_075',\n",
       " 'mobilenetv4_hybrid_medium',\n",
       " 'mobilenetv4_hybrid_medium_075',\n",
       " 'mobileone_s0',\n",
       " 'mobileone_s1',\n",
       " 'mobileone_s2',\n",
       " 'mobileone_s3',\n",
       " 'mobileone_s4',\n",
       " 'mobilevit_s',\n",
       " 'mobilevit_xs',\n",
       " 'mobilevit_xxs',\n",
       " 'mobilevitv2_050',\n",
       " 'mobilevitv2_075',\n",
       " 'mobilevitv2_100',\n",
       " 'mobilevitv2_125',\n",
       " 'mobilevitv2_150',\n",
       " 'mobilevitv2_175',\n",
       " 'mobilevitv2_200',\n",
       " 'mvitv2_base',\n",
       " 'mvitv2_base_cls',\n",
       " 'mvitv2_huge_cls',\n",
       " 'mvitv2_large',\n",
       " 'mvitv2_large_cls',\n",
       " 'mvitv2_small',\n",
       " 'mvitv2_small_cls',\n",
       " 'mvitv2_tiny',\n",
       " 'nasnetalarge',\n",
       " 'nest_base',\n",
       " 'nest_base_jx',\n",
       " 'nest_small',\n",
       " 'nest_small_jx',\n",
       " 'nest_tiny',\n",
       " 'nest_tiny_jx',\n",
       " 'nextvit_base',\n",
       " 'nextvit_large',\n",
       " 'nextvit_small',\n",
       " 'nf_ecaresnet26',\n",
       " 'nf_ecaresnet50',\n",
       " 'nf_ecaresnet101',\n",
       " 'nf_regnet_b0',\n",
       " 'nf_regnet_b1',\n",
       " 'nf_regnet_b2',\n",
       " 'nf_regnet_b3',\n",
       " 'nf_regnet_b4',\n",
       " 'nf_regnet_b5',\n",
       " 'nf_resnet26',\n",
       " 'nf_resnet50',\n",
       " 'nf_resnet101',\n",
       " 'nf_seresnet26',\n",
       " 'nf_seresnet50',\n",
       " 'nf_seresnet101',\n",
       " 'nfnet_f0',\n",
       " 'nfnet_f1',\n",
       " 'nfnet_f2',\n",
       " 'nfnet_f3',\n",
       " 'nfnet_f4',\n",
       " 'nfnet_f5',\n",
       " 'nfnet_f6',\n",
       " 'nfnet_f7',\n",
       " 'nfnet_l0',\n",
       " 'pit_b_224',\n",
       " 'pit_b_distilled_224',\n",
       " 'pit_s_224',\n",
       " 'pit_s_distilled_224',\n",
       " 'pit_ti_224',\n",
       " 'pit_ti_distilled_224',\n",
       " 'pit_xs_224',\n",
       " 'pit_xs_distilled_224',\n",
       " 'pnasnet5large',\n",
       " 'poolformer_m36',\n",
       " 'poolformer_m48',\n",
       " 'poolformer_s12',\n",
       " 'poolformer_s24',\n",
       " 'poolformer_s36',\n",
       " 'poolformerv2_m36',\n",
       " 'poolformerv2_m48',\n",
       " 'poolformerv2_s12',\n",
       " 'poolformerv2_s24',\n",
       " 'poolformerv2_s36',\n",
       " 'pvt_v2_b0',\n",
       " 'pvt_v2_b1',\n",
       " 'pvt_v2_b2',\n",
       " 'pvt_v2_b2_li',\n",
       " 'pvt_v2_b3',\n",
       " 'pvt_v2_b4',\n",
       " 'pvt_v2_b5',\n",
       " 'rdnet_base',\n",
       " 'rdnet_large',\n",
       " 'rdnet_small',\n",
       " 'rdnet_tiny',\n",
       " 'regnetv_040',\n",
       " 'regnetv_064',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_004_tv',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_008_tv',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_040_sgn',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_080_tv',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'regnety_640',\n",
       " 'regnety_1280',\n",
       " 'regnety_2560',\n",
       " 'regnetz_005',\n",
       " 'regnetz_040',\n",
       " 'regnetz_040_h',\n",
       " 'regnetz_b16',\n",
       " 'regnetz_b16_evos',\n",
       " 'regnetz_c16',\n",
       " 'regnetz_c16_evos',\n",
       " 'regnetz_d8',\n",
       " 'regnetz_d8_evos',\n",
       " 'regnetz_d32',\n",
       " 'regnetz_e8',\n",
       " 'repghostnet_050',\n",
       " 'repghostnet_058',\n",
       " 'repghostnet_080',\n",
       " 'repghostnet_100',\n",
       " 'repghostnet_111',\n",
       " 'repghostnet_130',\n",
       " 'repghostnet_150',\n",
       " 'repghostnet_200',\n",
       " 'repvgg_a0',\n",
       " 'repvgg_a1',\n",
       " 'repvgg_a2',\n",
       " 'repvgg_b0',\n",
       " 'repvgg_b1',\n",
       " 'repvgg_b1g4',\n",
       " 'repvgg_b2',\n",
       " 'repvgg_b2g4',\n",
       " 'repvgg_b3',\n",
       " 'repvgg_b3g4',\n",
       " 'repvgg_d2se',\n",
       " 'repvit_m0_9',\n",
       " 'repvit_m1',\n",
       " 'repvit_m1_0',\n",
       " 'repvit_m1_1',\n",
       " 'repvit_m1_5',\n",
       " 'repvit_m2',\n",
       " 'repvit_m2_3',\n",
       " 'repvit_m3',\n",
       " 'res2net50_14w_8s',\n",
       " 'res2net50_26w_4s',\n",
       " 'res2net50_26w_6s',\n",
       " 'res2net50_26w_8s',\n",
       " 'res2net50_48w_2s',\n",
       " 'res2net50d',\n",
       " 'res2net101_26w_4s',\n",
       " 'res2net101d',\n",
       " 'res2next50',\n",
       " 'resmlp_12_224',\n",
       " 'resmlp_24_224',\n",
       " 'resmlp_36_224',\n",
       " 'resmlp_big_24_224',\n",
       " 'resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet10t',\n",
       " 'resnet14t',\n",
       " 'resnet18',\n",
       " 'resnet18d',\n",
       " 'resnet26',\n",
       " 'resnet26d',\n",
       " 'resnet26t',\n",
       " 'resnet32ts',\n",
       " 'resnet33ts',\n",
       " 'resnet34',\n",
       " 'resnet34d',\n",
       " 'resnet50',\n",
       " 'resnet50_clip',\n",
       " 'resnet50_clip_gap',\n",
       " 'resnet50_gn',\n",
       " 'resnet50_mlp',\n",
       " 'resnet50c',\n",
       " 'resnet50d',\n",
       " 'resnet50s',\n",
       " 'resnet50t',\n",
       " 'resnet50x4_clip',\n",
       " 'resnet50x4_clip_gap',\n",
       " 'resnet50x16_clip',\n",
       " 'resnet50x16_clip_gap',\n",
       " 'resnet50x64_clip',\n",
       " 'resnet50x64_clip_gap',\n",
       " 'resnet51q',\n",
       " 'resnet61q',\n",
       " 'resnet101',\n",
       " 'resnet101_clip',\n",
       " 'resnet101_clip_gap',\n",
       " 'resnet101c',\n",
       " 'resnet101d',\n",
       " 'resnet101s',\n",
       " 'resnet152',\n",
       " 'resnet152c',\n",
       " 'resnet152d',\n",
       " 'resnet152s',\n",
       " 'resnet200',\n",
       " 'resnet200d',\n",
       " 'resnetaa34d',\n",
       " 'resnetaa50',\n",
       " 'resnetaa50d',\n",
       " 'resnetaa101d',\n",
       " 'resnetblur18',\n",
       " 'resnetblur50',\n",
       " 'resnetblur50d',\n",
       " 'resnetblur101d',\n",
       " 'resnetrs50',\n",
       " 'resnetrs101',\n",
       " 'resnetrs152',\n",
       " 'resnetrs200',\n",
       " 'resnetrs270',\n",
       " 'resnetrs350',\n",
       " 'resnetrs420',\n",
       " 'resnetv2_50',\n",
       " 'resnetv2_50d',\n",
       " 'resnetv2_50d_evos',\n",
       " 'resnetv2_50d_frn',\n",
       " 'resnetv2_50d_gn',\n",
       " 'resnetv2_50t',\n",
       " 'resnetv2_50x1_bit',\n",
       " 'resnetv2_50x3_bit',\n",
       " 'resnetv2_101',\n",
       " 'resnetv2_101d',\n",
       " 'resnetv2_101x1_bit',\n",
       " 'resnetv2_101x3_bit',\n",
       " 'resnetv2_152',\n",
       " 'resnetv2_152d',\n",
       " 'resnetv2_152x2_bit',\n",
       " 'resnetv2_152x4_bit',\n",
       " 'resnext26ts',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext50d_32x4d',\n",
       " 'resnext101_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_32x16d',\n",
       " 'resnext101_32x32d',\n",
       " 'resnext101_64x4d',\n",
       " 'rexnet_100',\n",
       " 'rexnet_130',\n",
       " 'rexnet_150',\n",
       " 'rexnet_200',\n",
       " 'rexnet_300',\n",
       " 'rexnetr_100',\n",
       " 'rexnetr_130',\n",
       " 'rexnetr_150',\n",
       " 'rexnetr_200',\n",
       " 'rexnetr_300',\n",
       " 'sam2_hiera_base_plus',\n",
       " 'sam2_hiera_large',\n",
       " 'sam2_hiera_small',\n",
       " 'sam2_hiera_tiny',\n",
       " 'samvit_base_patch16',\n",
       " 'samvit_base_patch16_224',\n",
       " 'samvit_huge_patch16',\n",
       " 'samvit_large_patch16',\n",
       " 'sebotnet33ts_256',\n",
       " 'sedarknet21',\n",
       " 'sehalonet33ts',\n",
       " 'selecsls42',\n",
       " 'selecsls42b',\n",
       " 'selecsls60',\n",
       " 'selecsls60b',\n",
       " 'selecsls84',\n",
       " 'semnasnet_050',\n",
       " 'semnasnet_075',\n",
       " 'semnasnet_100',\n",
       " 'semnasnet_140',\n",
       " 'senet154',\n",
       " 'sequencer2d_l',\n",
       " 'sequencer2d_m',\n",
       " 'sequencer2d_s',\n",
       " 'seresnet18',\n",
       " 'seresnet33ts',\n",
       " 'seresnet34',\n",
       " 'seresnet50',\n",
       " 'seresnet50t',\n",
       " 'seresnet101',\n",
       " 'seresnet152',\n",
       " 'seresnet152d',\n",
       " 'seresnet200d',\n",
       " 'seresnet269d',\n",
       " 'seresnetaa50d',\n",
       " 'seresnext26d_32x4d',\n",
       " 'seresnext26t_32x4d',\n",
       " 'seresnext26ts',\n",
       " 'seresnext50_32x4d',\n",
       " 'seresnext101_32x4d',\n",
       " 'seresnext101_32x8d',\n",
       " 'seresnext101_64x4d',\n",
       " 'seresnext101d_32x8d',\n",
       " 'seresnextaa101d_32x8d',\n",
       " 'seresnextaa201d_32x8d',\n",
       " 'skresnet18',\n",
       " 'skresnet34',\n",
       " 'skresnet50',\n",
       " 'skresnet50d',\n",
       " 'skresnext50_32x4d',\n",
       " 'spnasnet_100',\n",
       " 'swin_base_patch4_window7_224',\n",
       " 'swin_base_patch4_window12_384',\n",
       " 'swin_large_patch4_window7_224',\n",
       " 'swin_large_patch4_window12_384',\n",
       " 'swin_s3_base_224',\n",
       " 'swin_s3_small_224',\n",
       " 'swin_s3_tiny_224',\n",
       " 'swin_small_patch4_window7_224',\n",
       " 'swin_tiny_patch4_window7_224',\n",
       " 'swinv2_base_window8_256',\n",
       " 'swinv2_base_window12_192',\n",
       " 'swinv2_base_window12to16_192to256',\n",
       " 'swinv2_base_window12to24_192to384',\n",
       " 'swinv2_base_window16_256',\n",
       " 'swinv2_cr_base_224',\n",
       " 'swinv2_cr_base_384',\n",
       " 'swinv2_cr_base_ns_224',\n",
       " 'swinv2_cr_giant_224',\n",
       " 'swinv2_cr_giant_384',\n",
       " 'swinv2_cr_huge_224',\n",
       " 'swinv2_cr_huge_384',\n",
       " 'swinv2_cr_large_224',\n",
       " 'swinv2_cr_large_384',\n",
       " 'swinv2_cr_small_224',\n",
       " 'swinv2_cr_small_384',\n",
       " 'swinv2_cr_small_ns_224',\n",
       " 'swinv2_cr_small_ns_256',\n",
       " 'swinv2_cr_tiny_224',\n",
       " 'swinv2_cr_tiny_384',\n",
       " 'swinv2_cr_tiny_ns_224',\n",
       " 'swinv2_large_window12_192',\n",
       " 'swinv2_large_window12to16_192to256',\n",
       " 'swinv2_large_window12to24_192to384',\n",
       " 'swinv2_small_window8_256',\n",
       " 'swinv2_small_window16_256',\n",
       " 'swinv2_tiny_window8_256',\n",
       " 'swinv2_tiny_window16_256',\n",
       " 'test_byobnet',\n",
       " 'test_efficientnet',\n",
       " 'test_vit',\n",
       " 'tf_efficientnet_b0',\n",
       " 'tf_efficientnet_b1',\n",
       " 'tf_efficientnet_b2',\n",
       " 'tf_efficientnet_b3',\n",
       " 'tf_efficientnet_b4',\n",
       " 'tf_efficientnet_b5',\n",
       " 'tf_efficientnet_b6',\n",
       " 'tf_efficientnet_b7',\n",
       " 'tf_efficientnet_b8',\n",
       " 'tf_efficientnet_cc_b0_4e',\n",
       " 'tf_efficientnet_cc_b0_8e',\n",
       " 'tf_efficientnet_cc_b1_8e',\n",
       " 'tf_efficientnet_el',\n",
       " 'tf_efficientnet_em',\n",
       " 'tf_efficientnet_es',\n",
       " 'tf_efficientnet_l2',\n",
       " 'tf_efficientnet_lite0',\n",
       " 'tf_efficientnet_lite1',\n",
       " 'tf_efficientnet_lite2',\n",
       " 'tf_efficientnet_lite3',\n",
       " 'tf_efficientnet_lite4',\n",
       " 'tf_efficientnetv2_b0',\n",
       " 'tf_efficientnetv2_b1',\n",
       " 'tf_efficientnetv2_b2',\n",
       " 'tf_efficientnetv2_b3',\n",
       " 'tf_efficientnetv2_l',\n",
       " 'tf_efficientnetv2_m',\n",
       " 'tf_efficientnetv2_s',\n",
       " 'tf_efficientnetv2_xl',\n",
       " 'tf_mixnet_l',\n",
       " 'tf_mixnet_m',\n",
       " 'tf_mixnet_s',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100',\n",
       " 'tiny_vit_5m_224',\n",
       " 'tiny_vit_11m_224',\n",
       " 'tiny_vit_21m_224',\n",
       " 'tiny_vit_21m_384',\n",
       " 'tiny_vit_21m_512',\n",
       " 'tinynet_a',\n",
       " 'tinynet_b',\n",
       " 'tinynet_c',\n",
       " 'tinynet_d',\n",
       " 'tinynet_e',\n",
       " 'tnt_b_patch16_224',\n",
       " 'tnt_s_patch16_224',\n",
       " 'tresnet_l',\n",
       " 'tresnet_m',\n",
       " 'tresnet_v2_l',\n",
       " 'tresnet_xl',\n",
       " 'twins_pcpvt_base',\n",
       " 'twins_pcpvt_large',\n",
       " 'twins_pcpvt_small',\n",
       " 'twins_svt_base',\n",
       " 'twins_svt_large',\n",
       " 'twins_svt_small',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'visformer_small',\n",
       " 'visformer_tiny',\n",
       " 'vit_base_mci_224',\n",
       " 'vit_base_patch8_224',\n",
       " 'vit_base_patch14_dinov2',\n",
       " 'vit_base_patch14_reg4_dinov2',\n",
       " 'vit_base_patch16_18x2_224',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_miil',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch16_clip_224',\n",
       " 'vit_base_patch16_clip_384',\n",
       " 'vit_base_patch16_clip_quickgelu_224',\n",
       " 'vit_base_patch16_gap_224',\n",
       " 'vit_base_patch16_plus_240',\n",
       " 'vit_base_patch16_reg4_gap_256',\n",
       " 'vit_base_patch16_rope_reg1_gap_256',\n",
       " 'vit_base_patch16_rpn_224',\n",
       " 'vit_base_patch16_siglip_224',\n",
       " 'vit_base_patch16_siglip_256',\n",
       " 'vit_base_patch16_siglip_384',\n",
       " 'vit_base_patch16_siglip_512',\n",
       " 'vit_base_patch16_siglip_gap_224',\n",
       " 'vit_base_patch16_siglip_gap_256',\n",
       " 'vit_base_patch16_siglip_gap_384',\n",
       " 'vit_base_patch16_siglip_gap_512',\n",
       " 'vit_base_patch16_xp_224',\n",
       " 'vit_base_patch32_224',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_patch32_clip_224',\n",
       " 'vit_base_patch32_clip_256',\n",
       " 'vit_base_patch32_clip_384',\n",
       " 'vit_base_patch32_clip_448',\n",
       " 'vit_base_patch32_clip_quickgelu_224',\n",
       " 'vit_base_patch32_plus_256',\n",
       " 'vit_base_r26_s32_224',\n",
       " 'vit_base_r50_s16_224',\n",
       " 'vit_base_r50_s16_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_betwixt_patch16_gap_256',\n",
       " 'vit_betwixt_patch16_reg1_gap_256',\n",
       " 'vit_betwixt_patch16_reg4_gap_256',\n",
       " 'vit_betwixt_patch16_reg4_gap_384',\n",
       " 'vit_betwixt_patch16_rope_reg4_gap_256',\n",
       " 'vit_betwixt_patch32_clip_224',\n",
       " 'vit_giant_patch14_224',\n",
       " 'vit_giant_patch14_clip_224',\n",
       " 'vit_giant_patch14_dinov2',\n",
       " 'vit_giant_patch14_reg4_dinov2',\n",
       " 'vit_giant_patch16_gap_224',\n",
       " 'vit_gigantic_patch14_224',\n",
       " 'vit_gigantic_patch14_clip_224',\n",
       " 'vit_huge_patch14_224',\n",
       " 'vit_huge_patch14_clip_224',\n",
       " 'vit_huge_patch14_clip_336',\n",
       " 'vit_huge_patch14_clip_378',\n",
       " 'vit_huge_patch14_clip_quickgelu_224',\n",
       " 'vit_huge_patch14_clip_quickgelu_378',\n",
       " 'vit_huge_patch14_gap_224',\n",
       " 'vit_huge_patch14_xp_224',\n",
       " 'vit_huge_patch16_gap_448',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny-ViT 11M parameters: 10561442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TinyVit(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (conv1): ConvNorm(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act): GELU(approximate='none')\n",
       "    (conv2): ConvNorm(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (blocks): Sequential(\n",
       "        (0): MBConv(\n",
       "          (conv1): ConvNorm(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act2): GELU(approximate='none')\n",
       "          (conv3): ConvNorm(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act3): GELU(approximate='none')\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (conv1): ConvNorm(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act2): GELU(approximate='none')\n",
       "          (conv3): ConvNorm(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act3): GELU(approximate='none')\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TinyVitStage(\n",
       "      dim=128, depth=2\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=128, num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.018)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.018)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=128, num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.027)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.027)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TinyVitStage(\n",
       "      dim=256, depth=6\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=256, num_heads=8, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.036)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.036)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=256, num_heads=8, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.045)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.045)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TinyVitBlock(\n",
       "          dim=256, num_heads=8, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.055)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.055)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TinyVitBlock(\n",
       "          dim=256, num_heads=8, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.064)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.064)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): TinyVitBlock(\n",
       "          dim=256, num_heads=8, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.073)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.073)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): TinyVitBlock(\n",
       "          dim=256, num_heads=8, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.082)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.082)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TinyVitStage(\n",
       "      dim=448, depth=2\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(256, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(448, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=448, bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=448, num_heads=14, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
       "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=448, out_features=1792, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448, bias=False)\n",
       "            (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=448, num_heads=14, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
       "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=448, out_features=1792, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448, bias=False)\n",
       "            (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((448,), eps=1e-05, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=448, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model(\"tiny_vit_11m_224\", pretrained=True, num_classes=30)\n",
    "\n",
    "print(f\"Tiny-ViT 11M parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 2.0382, Train Acc: 51.31%\n",
      "Val Loss: 0.3851, Val Acc: 90.00%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.6656, Train Acc: 84.12%\n",
      "Val Loss: 0.4687, Val Acc: 86.84%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.4660, Train Acc: 88.80%\n",
      "Val Loss: 0.1387, Val Acc: 96.32%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.3566, Train Acc: 90.82%\n",
      "Val Loss: 0.0577, Val Acc: 97.89%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.3098, Train Acc: 92.27%\n",
      "Val Loss: 0.1048, Val Acc: 97.37%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.2477, Train Acc: 93.40%\n",
      "Val Loss: 0.0590, Val Acc: 97.63%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.2417, Train Acc: 93.64%\n",
      "Val Loss: 0.0786, Val Acc: 98.16%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.2156, Train Acc: 94.54%\n",
      "Val Loss: 0.0732, Val Acc: 97.89%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.1974, Train Acc: 94.54%\n",
      "Val Loss: 0.0081, Val Acc: 100.00%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.1720, Train Acc: 95.53%\n",
      "Val Loss: 0.0060, Val Acc: 99.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"tiny_vit_11m\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin-Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8808c1467fd747839af7bcfc645cbfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swin-Tiny parameters: 27542424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.018)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.027)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.036)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.045)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.045)\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.055)\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.064)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.064)\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.073)\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.082)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.082)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=30, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup and check model\n",
    "model = create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
    "in_features = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "print(f\"Swin-Tiny parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 3.2384, Train Acc: 7.87%\n",
      "Val Loss: 2.9876, Val Acc: 11.84%\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 3.3490, Train Acc: 7.25%\n",
      "Val Loss: 3.2785, Val Acc: 8.95%\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 3.2122, Train Acc: 8.80%\n",
      "Val Loss: 3.0631, Val Acc: 13.42%\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 3.1661, Train Acc: 8.52%\n",
      "Val Loss: 2.9981, Val Acc: 10.26%\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 3.1675, Train Acc: 8.87%\n",
      "Val Loss: 3.4485, Val Acc: 4.21%\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 3.2146, Train Acc: 9.86%\n",
      "Val Loss: 3.1088, Val Acc: 10.26%\n",
      "\n",
      "Epoch 7/10:\n",
      "Train Loss: 3.1785, Train Acc: 9.21%\n",
      "Val Loss: 3.0609, Val Acc: 13.95%\n",
      "\n",
      "Epoch 8/10:\n",
      "Train Loss: 3.1492, Train Acc: 10.52%\n",
      "Val Loss: 3.1975, Val Acc: 11.32%\n",
      "\n",
      "Epoch 9/10:\n",
      "Train Loss: 3.3096, Train Acc: 7.66%\n",
      "Val Loss: 3.1608, Val Acc: 10.00%\n",
      "\n",
      "Epoch 10/10:\n",
      "Train Loss: 3.2006, Train Acc: 9.55%\n",
      "Val Loss: 3.0651, Val Acc: 11.32%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "results_log.append(train_model(model, \"Swin-Tiny\", trainloader, valloader, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
